\BOOKMARK [0][-]{chapter.1}{Introduction to Reinforcement Learning}{}% 1
\BOOKMARK [1][-]{section.1.1}{What is Reinforcement Learning?}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Some Terminology}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{The Reward Hypothesis}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.4}{Exploration vs Exploitation}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.5}{Classical Methods}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.5.1}{Game Trees}{section.1.5}% 7
\BOOKMARK [2][-]{subsection.1.5.2}{The Minimax Algorithm}{section.1.5}% 8
\BOOKMARK [1][-]{section.1.6}{The Reinforcement Learning Method}{chapter.1}% 9
\BOOKMARK [0][-]{chapter.2}{Multiarmed Bandits}{}% 10
\BOOKMARK [1][-]{section.2.1}{Definitions}{chapter.2}% 11
\BOOKMARK [1][-]{section.2.2}{Balancing Exploration and Exploitation}{chapter.2}% 12
\BOOKMARK [2][-]{subsection.2.2.1}{The Greedy Policy}{section.2.2}% 13
\BOOKMARK [2][-]{subsection.2.2.2}{}{section.2.2}% 14
\BOOKMARK [2][-]{subsection.2.2.3}{Upper Confidence Bound Action Selection}{section.2.2}% 15
\BOOKMARK [2][-]{subsection.2.2.4}{Gradient Bandits}{section.2.2}% 16
\BOOKMARK [2][-]{subsection.2.2.5}{Thompson Sampling}{section.2.2}% 17
\BOOKMARK [1][-]{section.2.3}{Comparing Policies}{chapter.2}% 18
\BOOKMARK [1][-]{section.2.4}{Contextual Bandits}{chapter.2}% 19
\BOOKMARK [0][-]{chapter.3}{Markov Decision Processes}{}% 20
\BOOKMARK [1][-]{section.3.1}{Formalization of the Reinforcement Learning Problem}{chapter.3}% 21
\BOOKMARK [1][-]{section.3.2}{Return and Discounting}{chapter.3}% 22
\BOOKMARK [1][-]{section.3.3}{Markov Decision Processes and the Markov Property}{chapter.3}% 23
\BOOKMARK [1][-]{section.3.4}{The Bellman Equations}{chapter.3}% 24
\BOOKMARK [1][-]{section.3.5}{Bellman Optimality Equations}{chapter.3}% 25
\BOOKMARK [1][-]{section.3.6}{Linear Programming}{chapter.3}% 26
\BOOKMARK [1][-]{section.3.7}{Difficulties in Linear Programming}{chapter.3}% 27
\BOOKMARK [0][-]{chapter.4}{Dynamic Programming}{}% 28
\BOOKMARK [1][-]{section.4.1}{Introduction}{chapter.4}% 29
\BOOKMARK [1][-]{section.4.2}{Policy Evaluation}{chapter.4}% 30
\BOOKMARK [1][-]{section.4.3}{Policy Improvement}{chapter.4}% 31
\BOOKMARK [2][-]{subsection.4.3.1}{The Policy Improvement Theorem}{section.4.3}% 32
\BOOKMARK [2][-]{subsection.4.3.2}{Improving our Policy}{section.4.3}% 33
\BOOKMARK [1][-]{section.4.4}{Policy Iteration}{chapter.4}% 34
\BOOKMARK [1][-]{section.4.5}{Value Iteration}{chapter.4}% 35
\BOOKMARK [1][-]{section.4.6}{Asynchronous Dynamic Programming}{chapter.4}% 36
\BOOKMARK [1][-]{section.4.7}{Prioritized Sweeping}{chapter.4}% 37
\BOOKMARK [1][-]{section.4.8}{Generalized Policy Iteration}{chapter.4}% 38
\BOOKMARK [0][-]{chapter.5}{Monte Carlo Methods}{}% 39
\BOOKMARK [1][-]{section.5.1}{Estimating the State Value Function}{chapter.5}% 40
\BOOKMARK [1][-]{section.5.2}{Estimating the Action Value Function}{chapter.5}% 41
\BOOKMARK [1][-]{section.5.3}{Problems with Visit Based Estimation}{chapter.5}% 42
\BOOKMARK [2][-]{subsection.5.3.1}{Exploring Starts}{section.5.3}% 43
\BOOKMARK [1][-]{section.5.4}{Monte Carlo ES}{chapter.5}% 44
\BOOKMARK [1][-]{section.5.5}{Removing the Exploring Starts Assumption}{chapter.5}% 45
\BOOKMARK [2][-]{subsection.5.5.1}{}{section.5.5}% 46
\BOOKMARK [2][-]{subsection.5.5.2}{Off Policy Estimation: Importance Sampling}{section.5.5}% 47
