\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to Reinforcement Learning}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What is Reinforcement Learning?}{3}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Some Terminology}{3}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}The Reward Hypothesis}{4}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Exploration vs Exploitation}{4}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Classical Methods}{4}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Game Trees}{5}{subsection.1.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The first three levels of the game tree of Tic Tac Toe (upto symmetry)}}{5}{figure.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}The Minimax Algorithm}{7}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}The Reinforcement Learning Method}{8}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Multiarmed Bandits}{10}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Definitions}{10}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Balancing Exploration and Exploitation}{11}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Greedy Policy}{12}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Here we have prepared a testbed of 100 identical 10 armed bandits, and we test multiple strategies on this testbed. \texttt  {\% Optimal Action} refers to the percentage of bandits in the testbed choosing the action with the highest value. One can see that the optimistic greedy policy sometimes outperforms realistic versions of even non-greedy algorithms.}}{12}{figure.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The $\varepsilon $-greedy Policy}{13}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces We again use the same testbed described earlier and compare greedy and $\varepsilon $-greedy policies. \texttt  {Average Reward} here refers to the average reward each agent obtained at a particular timestep. As we can see, the $\varepsilon $-greedy policies perform better than the greedy policies. Among the $\varepsilon $-greedy policies, the one with the lower $\varepsilon $ learns slower, but asymptotically performs better than the one with higher $\varepsilon $.}}{13}{figure.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Upper Confidence Bound Action Selection}{13}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces We use the same testbed again to compare the UCBA and $\varepsilon $-greedy policies. As you can see, the UCBA policy is better.}}{14}{figure.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Gradient Bandits}{14}{subsection.2.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces We use the previously discussed testbed to compare different types of Gradient Ascent algorithms.}}{15}{figure.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Thompson Sampling}{16}{subsection.2.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Comparing Policies}{16}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Contextual Bandits}{16}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces We compare all the different policies on our testbed over different values for their parameters. $\texttt  {Average Reward}$ here refers to the average reward obtained over the first $1000$ timesteps. The characteristic inverted U shape of these curves shows that an optimal value exists for the parameters.}}{17}{figure.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Markov Decision Processes}{18}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Formalization of the Reinforcement Learning Problem}{18}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Return and Discounting}{18}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Markov Decision Processes and the Markov Property}{19}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}The Bellman Equations}{21}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Bellman Optimality Equations}{22}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Linear Programming}{23}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Difficulties in Linear Programming}{23}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Dynamic Programming}{25}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{25}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Policy Evaluation}{25}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Policy Improvement}{27}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}The Policy Improvement Theorem}{27}{subsection.4.3.1}\protected@file@percent }
\newlabel{subsec:PIT}{{4.3.1}{27}{The Policy Improvement Theorem}{subsection.4.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Improving our Policy}{28}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Policy Iteration}{28}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Value Iteration}{29}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Asynchronous Dynamic Programming}{29}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Prioritized Sweeping}{30}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Generalized Policy Iteration}{30}{section.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Monte Carlo Methods}{31}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Estimating the State Value Function}{31}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Estimating the Action Value Function}{32}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Problems with Visit Based Estimation}{32}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Exploring Starts}{32}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Monte Carlo ES}{32}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Removing the Exploring Starts Assumption}{33}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}On-Policy Methods: $\varepsilon $-soft policies}{33}{subsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Off Policy Estimation: Importance Sampling}{35}{subsection.5.5.2}\protected@file@percent }
